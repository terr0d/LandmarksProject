### Задание 1
Принять решение, какие данные - плохие, придумать, как почистить датасет не руками, а моделями, возможно, при помощи opencv. Реализовать очистку датасета.

---
Судя по заданиям, финальный продукт будет чем-то похожим на умный электронный гид, распознающий достопримечательности по фото, сделанному туристом, и выдающий фото достопримечательностей по текстовому запросу.

Соответсвенно, к основным источникам плохих данных в датасете можно отнести: 
- Фотографии, полностью не соответствующие достопримечательностям
- Фото с цветовыми артефактами (ЧБ или сепия в основном цветном датасете)
- Коллажи из фотографий достопримечательностей (даже одной и той же)
- Фотографии внутри зданий достопримечательностей (на мой взгляд, это не соответствует юз-кейсу, так как турист скорее захочет узнать про достопримечательность, гуляя по улице), но могут быть исключения

Главной задачей является автоматизированное удаление большей части выбросов, но, при этом, пытаясь минимизировать FP классификации.

В своем решении я применяю 2 подхода:
- Эмбеддинги через [CLIP](https://huggingface.co/openai/clip-vit-base-patch32), затем кластеризация внутри достопримечательностей, используя DBSCAN (хорошо определяет выбросы). В результате, лучше всего отсеиваются семантически не соответствующие фото, а также часть коллажей и довольно мало малоцветных фото.
- OpenCV для определения черно-белых фото. В датасете также бывает сепия, но, пробуя разные подходы, всегда получалось довольно много FP прогнозов, чего почти не было для ЧБ.

Важным моментом является создание сначала временной папки со всеми кандидатами на удаление (в самом начале для того, чтобы посмотреть на адекватность классификации выбросов в целом, а на более поздних итерациях - чтобы убирать как можно больше FP классификаций), а затем удаление в основном датасете только тех фото, которые есть во временной папке.

Помоги сделать так, чтобы не было много сплошного текста в списках, не сильно меняя текст, в идеале только форматирование

### Задание 2
По изображению определять топ-5 наиболее подходящих названий и топ-5 категорий достопримечательностей.

---
**Основная идея**: изображения из очищенного датасета переводим в CLIP-эмбеддинги, кэшируем и связываем их с сырыми данными о достопримечательностях (из файлов places.csv), на инференсе получаем эмбеддинг переданного изображения и ищем 20 наиболее близких к нему векторов в заранее построенном индексе, среди которых, с учетом их близости (веса по сходству) определяем топ-5 названий и категорий. Таким образом, задача классификации сводится к нахождению k‑ближайших соседей в пространстве CLIP‑эмбеддингов.

**Стек**:
- [clip-ViT-B-32](https://huggingface.co/sentence-transformers/clip-ViT-B-32) (через [SentenceTransformer](https://github.com/huggingface/sentence-transformers)) - используется как визуальный энкодер для получения эмбеддингов из изображений датасета и на инференсе. Была выбрана для связки с мультиязычной [clip-ViT-B-32-multilingual-v1](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1), обученной так, чтобы текстовые эмбеддинги жили в том же пространстве, что и визуальные.
- [FAISS](https://github.com/facebookresearch/faiss) - используется для быстрого поиска похожих на целевой векторов по их косинусному сходству (скалярное произведение нормализованных векторов). Индекс строится один раз по всем изображениям и затем используется как для задачи img2text, так и для text2img

### Задание 3
Реализовать поиск изображений по тексту. По входному запросу возвращать топ-5 наиболее релевантных изображений.

---
**Основная идея**: поскольку задача зеркальная, используем тот же подход с общим векторным пространством, но теперь переводим туда текстовый запрос и ищем 5 наиболее близких к нему эмбеддингов изображений.

**Стек**:
- [clip-ViT-B-32-multilingual-v1](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1) (через [SentenceTransformer](https://github.com/huggingface/sentence-transformers)) - используется как текстовый энкодер на инференсе. Был выбран из-за поддержки множества языков (в том числе и русского), что хорошо соответствует юз‑кейсу задания. За счёт этого текстовые мультиязычные и визуальные эмбеддинги попадают в одно и то же пространство, что позволяет искать изображения по тексту и использовать те же эмбеддинги изображений для решения задачи 2 без дополнительного переобучения. Если же продукт разрабатывается чисто для внутреннего рынка, тогда можно попробовать ruCLIP.
- [FAISS](https://github.com/facebookresearch/faiss) - тот же индекс, что и в задании 2.